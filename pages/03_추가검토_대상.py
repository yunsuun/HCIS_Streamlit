import streamlit as st
import numpy as np
import pandas as pd

from pathlib import Path

from config import (
    APP_TITLE, ID_COL, OFFSET, FACTOR, T_LOW, T_HIGH,
    MODEL_DF_PARQUET, MAPPING_PATH, TOP_N
)

from utils.hcis_core import build_map_dict, build_payload_from_team_row, compute_hcis_columns
from utils.shap_reason import get_top_reasons_from_shap_row
from utils.risk_types import (
    RISK_TYPES,
    classify_review_payload,
    risk_type_display,
    risk_type_guidance,
)
from utils.review_simulation import SimParams, simulate_type_based_conversion, summarize_candidates_by_type



# -----------------------------------------------------------
# Page config
# -----------------------------------------------------------
st.set_page_config(page_title=f"{APP_TITLE} | ì¶”ê°€ê²€í†  í™•ì¸", layout="wide")

st.markdown(
    """
<style>
.block-container { padding-top: 2rem !important; }
.small-muted { color: #666; font-size: 12px; }
</style>
""",
    unsafe_allow_html=True,
)

st.title("ğŸŸ¡ ì¶”ê°€ê²€í†  í™•ì¸ ì„¼í„°")
st.caption("ì¶”ê°€ê²€í†  êµ¬ê°„ ê³ ê°ì„ 'ì ìˆ˜ ì¤„ì„¸ìš°ê¸°'ê°€ ì•„ë‹ˆë¼ 'ë¦¬ìŠ¤í¬ íƒ€ì…'ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , 2ì°¨ í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")


# -----------------------------------------------------------
# Data load
# -----------------------------------------------------------
@st.cache_data(ttl=3600, show_spinner="ë°ì´í„° ë¡œë”© ì¤‘...")
def load_df_work(data_path: Path) -> pd.DataFrame:
    df = pd.read_parquet(data_path)
    df[ID_COL] = df[ID_COL].astype(str)
    return df

ST_DATA_DF_PARQUET = Path("st_data") / "model_df.parquet"

if ST_DATA_DF_PARQUET.exists():
    data_src = f"st_data ({ST_DATA_DF_PARQUET.as_posix()})"
    df_work = load_df_work(ST_DATA_DF_PARQUET)
else:
    data_src = f"config ({Path(MODEL_DF_PARQUET).as_posix()})"
    df_work = load_df_work(Path(MODEL_DF_PARQUET))

st.caption(f"ë°ì´í„° ì†ŒìŠ¤: `{data_src}`")

# HCIS ì»¬ëŸ¼ ë³´ì •
if ("hcis_score" not in df_work.columns) or ("band" not in df_work.columns):
    df_work = compute_hcis_columns(df_work, pd_col="pd_hat")

# -----------------------------------------------------------
# Filter: Review band
# -----------------------------------------------------------
df_review = df_work[df_work["band"] == "ì¶”ê°€ê²€í† "].copy()

# ìƒë‹¨ KPI
c1, c2, c3, c4 = st.columns(4)
with c1:
    st.metric("ì „ì²´ ê³ ê°", f"{len(df_work):,}")
with c2:
    st.metric("ì¶”ê°€ê²€í†  ê³ ê°", f"{len(df_review):,}")
with c3:
    rate = (len(df_review) / max(len(df_work), 1)) * 100
    st.metric("ì¶”ê°€ê²€í†  ë¹„ì¤‘", f"{rate:.2f}%")
with c4:
    if len(df_review) > 0:
        st.metric("ì¶”ê°€ê²€í†  í‰ê·  HCIS", f"{df_review['hcis_score'].mean():.1f}")
    else:
        st.metric("ì¶”ê°€ê²€í†  í‰ê·  HCIS", "-")

if df_review.empty:
    st.info("ì¶”ê°€ê²€í†  ê³ ê°ì´ ì—†ìŠµë‹ˆë‹¤. ê°œìš”ì—ì„œ ì—…ë¡œë“œ/ì¶”ë¡  í›„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()


# -----------------------------------------------------------
# Mapping + Classification (cache)
# -----------------------------------------------------------
@st.cache_resource
def get_map_dict_cached(mapping_path: str):
    return build_map_dict(Path(mapping_path))

@st.cache_data(show_spinner="ì¶”ê°€ê²€í†  ê³ ê° ë¶„ë¥˜ ì¤‘...")
def classify_review_rows(df: pd.DataFrame, mapping_path: str) -> pd.DataFrame:
    map_dict = get_map_dict_cached(mapping_path)

    rows = []
    for _, r in df.iterrows():
        row_series = r  # pd.Series

        # payload: SHAP bundle + group contributionê¹Œì§€ í¬í•¨
        payload = build_payload_from_team_row(
            row=row_series,
            map_dict=map_dict,
            id_col=ID_COL,
            t_low=T_LOW,
            t_high=T_HIGH,
            offset=OFFSET,
            factor=FACTOR,
            top_n_use=TOP_N,
            top_features_col="shap_features",
            top_values_col="shap_values",
        )

        rt_key, dbg = classify_review_payload(payload)

        # UIìš© top reasons (ê°„ë‹¨ ë¬¸ì¥ 10ê°œ)
        reasons_txt = get_top_reasons_from_shap_row(
            row_series,
            map_dict,
            top_k=TOP_N,
            top_features_col="shap_features",
            top_values_col="shap_values",
            only_risk_positive=True,
        )

        rows.append(
            {
                "sk_id_curr": str(row_series.get(ID_COL)),
                "hcis_score": float(payload.get("hcis_score", np.nan)),
                "margin_score": float(payload.get("policy", {}).get("margin_score", np.nan)),
                "pd_hat": float(payload.get("pd_hat", np.nan)),
                "risk_type_key": rt_key,
                "risk_type": risk_type_display(rt_key),
                "dominant_group": dbg.get("dominant_group"),
                "credit_pct": dbg.get("credit_pct"),
                "docs_pct": dbg.get("docs_pct"),
                "capacity_pct": dbg.get("capacity_pct"),
                "emp_pct": dbg.get("emp_pct"),
                "top_reasons": " / ".join(reasons_txt) if reasons_txt else "",
            }
        )

    out = pd.DataFrame(rows)

    # ì •ë ¬: ë§ˆì§„ í° ìˆœ(ìŠ¹ì¸ì— ë” ê°€ê¹Œìš´ ì¶”ê°€ê²€í† ) ìš°ì„ 
    if "margin_score" in out.columns:
        out = out.sort_values("margin_score", ascending=False, na_position="last")

    return out


df_classified = classify_review_rows(df_review[[c for c in df_review.columns]].copy(), MAPPING_PATH)

st.markdown("---")
st.subheader("ğŸ“ˆ ì¶”ê°€ê²€í†  ìŠ¹ì¸ ì „í™˜ ì‹œë®¬ë ˆì´ì…˜ (Risk Type ê¸°ë°˜)")

# df_classifiedì—ëŠ” risk_type_keyê°€ ìˆê³ , pd_hat / (ìˆìœ¼ë©´ target) ë„ ìˆìŒ
# í›„ë³´ íƒ€ì… ê¸°ë³¸ê°’: Type2/Type3/Type4
default_types = ["TYPE2_DOCS_UNCERTAINTY", "TYPE3_SPENDING_IMBALANCE", "TYPE4_EMPLOYMENT_LIFECYCLE"]

with st.expander("ì„¤ì • / ê°€ì •", expanded=True):
    c1, c2, c3 = st.columns(3)
    with c1:
        ead = st.number_input("EAD(ê±´ë‹¹ ëŒ€ì¶œì›ê¸ˆ, ì›)", min_value=0, value=5_000_000, step=100_000)
    with c2:
        apr = st.number_input("APR(ì—° ì´ììœ¨)", min_value=0.0, value=0.12, step=0.01, format="%.2f")
    with c3:
        tenor = st.number_input("ê¸°ê°„(ê°œì›”)", min_value=1, value=12, step=1)

    c4, c5 = st.columns(2)
    with c4:
        lgd = st.number_input("LGD(ì†ì‹¤ë¥ )", min_value=0.0, value=0.60, step=0.05, format="%.2f")
    with c5:
        review_cost = st.number_input("ì¶”ê°€ê²€í†  ìš´ì˜ë¹„ìš©(í›„ë³´ 1ê±´ë‹¹, ì›)", min_value=0, value=10_000, step=1_000)

    type_options = list(RISK_TYPES.keys())
    include_types = st.multiselect(
        "ìŠ¹ì¸ ì „í™˜ í›„ë³´ íƒ€ì…(í™•ì¸ìœ¼ë¡œ í•´ì†Œ ê°€ëŠ¥í•œ ìœ í˜•ì„ ì„ íƒ)",
        options=type_options,
        default=[t for t in default_types if t in type_options],
    )

    conv_rates = st.multiselect(
        "í™•ì¸ ì„±ê³µë¥  ì‹œë‚˜ë¦¬ì˜¤(í›„ë³´ ì¤‘ ìŠ¹ì¸ ì „í™˜ ë¹„ìœ¨)",
        options=[0.1, 0.2, 0.3, 0.5, 0.7, 0.9],
        default=[0.3, 0.5, 0.7],
    )

params = SimParams(
    ead=float(ead),
    apr=float(apr),
    tenor_months=int(tenor),
    lgd=float(lgd),
    review_cost_per_case=float(review_cost),
    target_col="target" if "target" in df_review.columns else None
)

# íƒ€ì…ë³„ í›„ë³´ í˜„í™© ìš”ì•½
st.markdown("#### í›„ë³´ íƒ€ì… í˜„í™©(ì¶”ê°€ê²€í†  ë‚´)")
cand_summary = summarize_candidates_by_type(df_review.merge(
    df_classified[[ID_COL, "risk_type_key"]], on=ID_COL, how="left"
), type_col="risk_type_key", pd_col="pd_hat")
st.dataframe(cand_summary, use_container_width=True, hide_index=True)

# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
if st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰", type="primary"):
    # df_review(ì›ë³¸) + risk_type_key ì¡°ì¸
    df_for_sim = df_review.merge(
        df_classified[[ID_COL, "risk_type_key"]],
        on=ID_COL,
        how="left"
    )

    res = simulate_type_based_conversion(
        df_for_sim,
        include_types=include_types,
        conv_rates=sorted(conv_rates),
        params=params,
        pd_col="pd_hat",
        type_col="risk_type_key",
    )

    st.markdown("#### ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼")
    st.dataframe(res, use_container_width=True, hide_index=True)

    # í•µì‹¬ KPI í•œ ì¤„ ìš”ì•½
    best = res.sort_values("net_profit", ascending=False).iloc[0]
    st.success(
        f"ê°€ì¥ ë†’ì€ ìˆœì´ìµ ì‹œë‚˜ë¦¬ì˜¤: {best['scenario']} Â· "
        f"ì „í™˜ {int(best['n_converted']):,}ëª… / í›„ë³´ {int(best['n_candidates']):,}ëª… Â· "
        f"ìˆœì´ìµ {best['net_profit']:,.0f}ì›"
    )

# -----------------------------------------------------------
# Type ë¶„í¬
# -----------------------------------------------------------
st.markdown("---")
st.subheader("ğŸ“Œ ì¶”ê°€ê²€í†  ë¦¬ìŠ¤í¬ íƒ€ì… ë¶„í¬")

counts = df_classified["risk_type"].value_counts().reset_index()
counts.columns = ["risk_type", "n"]

# bar chart
import altair as alt

chart = (
    alt.Chart(counts)
    .mark_bar()
    .encode(
        y=alt.Y(
            "risk_type:N",
            title="Risk Type",
            sort="-x",
            axis=alt.Axis(
                labelLimit=300,      # ê¸€ì ì˜ë¦¼ ë°©ì§€ (í•µì‹¬)
                labelFontSize=12
            )
        ),
        x=alt.X("n:Q", title="ê³ ê° ìˆ˜"),
        tooltip=["risk_type:N", "n:Q"]
    )
    .properties(
        height=320,
        padding={"left": 30}     # ì™¼ìª½ ì—¬ë°± ê°•ì œ í™•ë³´ (í•µì‹¬)
    )
)

st.altair_chart(chart, use_container_width=True)




# -----------------------------------------------------------
# Filter controls
# -----------------------------------------------------------
st.markdown("---")
st.subheader("ğŸ” íƒ€ì…ë³„ í›„ë³´ ì¡°íšŒ")

col_f1, col_f2, col_f3 = st.columns([2, 2, 2])
with col_f1:
    type_options = ["ì „ì²´"] + sorted(df_classified["risk_type"].dropna().unique().tolist())
    sel_type = st.selectbox("Risk Type", type_options, index=0)
with col_f2:
    min_hcis = float(df_classified["hcis_score"].min())
    max_hcis = float(df_classified["hcis_score"].max())

    if np.isclose(min_hcis, max_hcis):
        st.info(f"HCISê°€ ë‹¨ì¼ ê°’ì…ë‹ˆë‹¤: {min_hcis:.2f}")
        hcis_range = (min_hcis, max_hcis)
    else:
        hcis_range = st.slider(
            "HCIS ë²”ìœ„",
            min_value=min_hcis,
            max_value=max_hcis,
            value=(min_hcis, max_hcis),
        )

with col_f3:
    # ë§ˆì§„ì€ ìŒìˆ˜~ì–‘ìˆ˜ ì„ì„
    ms = pd.to_numeric(df_classified["margin_score"], errors="coerce")
    min_m = float(np.nanmin(ms))
    max_m = float(np.nanmax(ms))

    # msê°€ ì „ë¶€ NaNì´ê±°ë‚˜, ë‹¨ì¼ ê°’ì´ë©´ slider ëŒ€ì‹  ê³ ì •
    if (not np.isfinite(min_m)) or (not np.isfinite(max_m)):
        st.info("ë§ˆì§„ ê°’ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        margin_range = (-np.inf, np.inf)
    elif np.isclose(min_m, max_m):
        st.info(f"ë§ˆì§„ì´ ë‹¨ì¼ ê°’ì…ë‹ˆë‹¤: {min_m:.2f}")
        margin_range = (min_m, max_m)
    else:
        margin_range = st.slider(
            "ë§ˆì§„ ë²”ìœ„(cutoff ëŒ€ë¹„)",
            min_value=min_m,
            max_value=max_m,
            value=(min_m, max_m),
        )


filtered = df_classified.copy()
if sel_type != "ì „ì²´":
    filtered = filtered[filtered["risk_type"] == sel_type]
filtered = filtered[(filtered["hcis_score"] >= hcis_range[0]) & (filtered["hcis_score"] <= hcis_range[1])]
filtered = filtered[(filtered["margin_score"] >= margin_range[0]) & (filtered["margin_score"] <= margin_range[1])]

st.caption(f"í•„í„° ê²°ê³¼: {len(filtered):,}ëª…")

# -----------------------------------------------------------
# Candidate table + drilldown
# -----------------------------------------------------------
show_cols = [
    ID_COL,
    "hcis_score",
    "margin_score",
    "pd_hat",
    "risk_type",
    "dominant_group",
    "top_reasons",
]

st.dataframe(
    filtered[show_cols],
    use_container_width=True,
    hide_index=True,
)

st.markdown("<div class='small-muted'>Tip: ì•„ë˜ì—ì„œ ê³ ê° IDë¥¼ ì„ íƒí•˜ë©´, ê°œì¸ ì‹¬ì‚¬ í˜ì´ì§€ì—ì„œ í•´ë‹¹ IDë¡œ ë°”ë¡œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</div>", unsafe_allow_html=True)

# -----------------------------------------------------------
# Drilldown: select one customer
# -----------------------------------------------------------
left, right = st.columns([2, 3])

with left:
    st.markdown("#### ğŸ‘¤ ê³ ê° ì„ íƒ")
    ids = filtered[ID_COL].astype(str).unique().tolist()
    sel_id = st.selectbox("ê³ ê° ID", ids) if ids else None

    if sel_id:
        st.code(f"ì„ íƒ ê³ ê° ID: {sel_id}")
        st.markdown("- ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë™ì¼ IDë¥¼ ì…ë ¥í•˜ë©´ **'ëŒ€ì¶œ ì‹¬ì‚¬ ì¡°íšŒ'** í˜ì´ì§€ì—ì„œ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.")

with right:
    st.markdown("#### ğŸ§­ 2ì°¨ í‰ê°€ ê°€ì´ë“œ")
    if not sel_id:
        st.info("ì™¼ìª½ì—ì„œ ê³ ê°ì„ ì„ íƒí•´ì£¼ì„¸ìš”")
    else:
        row_sel = filtered[filtered[ID_COL] == str(sel_id)].iloc[0]
        rt_key = row_sel["risk_type_key"]
        spec = RISK_TYPES.get(rt_key)
        guide = risk_type_guidance(rt_key)

        st.markdown(f"**{spec.name if spec else rt_key}**")
        st.write(spec.short_desc if spec else "")

        cqa, cact = st.columns(2)
        with cqa:
            st.markdown("**âœ… í™•ì¸ ì§ˆë¬¸(ì²´í¬ë¦¬ìŠ¤íŠ¸)**")
            if guide["checklist_questions"]:
                for q in guide["checklist_questions"]:
                    st.write(f"- {q}")
            else:
                st.write("- (ì •ì˜ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤)")

        with cact:
            st.markdown("**ğŸ› ï¸ ê¶Œì¥ ì•¡ì…˜(ì‹¬ì‚¬/ìš´ì˜)**")
            if guide["suggested_actions"]:
                for a in guide["suggested_actions"]:
                    st.write(f"- {a}")
            else:
                st.write("- (ì •ì˜ëœ ì•¡ì…˜ì´ ì—†ìŠµë‹ˆë‹¤)")

        st.markdown("---")
        st.markdown("**ğŸ“ ì„ íƒ ê³ ê° í•µì‹¬ ì§€í‘œ**")
        k1, k2, k3 = st.columns(3)
        k1.metric("HCIS", f"{row_sel['hcis_score']:.0f}")
        k2.metric("ë§ˆì§„", f"{row_sel['margin_score']:+.1f}")
        k3.metric("PD_hat", f"{row_sel['pd_hat']:.4f}")

        if row_sel.get("top_reasons"):
            st.markdown("**ğŸ” ì£¼ìš” ì°¸ê³  ìš”ì¸(Top10)**")
            for i, t in enumerate(str(row_sel["top_reasons"]).split(" / "), 1):
                st.write(f"{i}. {t}")

